{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\uamaha01\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_img, img_to_array\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minception_v3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InceptionV3, preprocess_input \u001b[38;5;28;01mas\u001b[39;00m inception_preprocess\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet50\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet50, preprocess_input \u001b[38;5;28;01mas\u001b[39;00m resnet_preprocess\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\uamaha01\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_preprocess\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as resnet_preprocess\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Encoder ready. Output shape: (None, 2048)\n"
     ]
    }
   ],
   "source": [
    "# cell: choose encoder\n",
    "encoder_name = 'inceptionv3'   # change to 'resnet50' if you prefer\n",
    "\n",
    "if encoder_name.lower() == 'inceptionv3':\n",
    "    cnn_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    preprocess_fn = inception_preprocess\n",
    "    target_size = (299, 299)\n",
    "elif encoder_name.lower() == 'resnet50':\n",
    "    cnn_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    preprocess_fn = resnet_preprocess\n",
    "    target_size = (224, 224)\n",
    "else:\n",
    "    raise ValueError(\"encoder_name must be 'inceptionv3' or 'resnet50'\")\n",
    "\n",
    "# This model returns a fixed-size vector per image (e.g. 2048)\n",
    "print(\"Encoder ready. Output shape:\", cnn_model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape: (1, 2048)\n"
     ]
    }
   ],
   "source": [
    "# cell: test feature extraction on one image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "# ...existing code...\n",
    "sample_img_path = r\"C:\\Users\\uamaha01\\Downloads\\AI project\\Images\\99171998_7cc800ceef.jpg\"  # <- change to an actual path\n",
    "# ...existing code... \n",
    "\n",
    "img = load_img(sample_img_path, target_size=target_size)\n",
    "x  = img_to_array(img)\n",
    "x  = np.expand_dims(x, axis=0)\n",
    "x  = preprocess_fn(x)            # model-specific preprocessing\n",
    "feat = cnn_model.predict(x, verbose=0)\n",
    "print(\"feature shape:\", feat.shape)   # expect (1, 2048) or (1, 512) depending on model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_dir(image_dir, out_file='features_inception.npz'):\n",
    "    files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "    files.sort()\n",
    "    features = {}\n",
    "    for fname in tqdm(files, desc=\"Extracting\"):\n",
    "        img_id = os.path.splitext(fname)[0]\n",
    "        p = os.path.join(image_dir, fname)\n",
    "        img = load_img(p, target_size=target_size)\n",
    "        x = img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_fn(x)\n",
    "        feat = cnn_model.predict(x, verbose=0).squeeze()\n",
    "        features[img_id] = feat\n",
    "    # save compressed\n",
    "    np.savez_compressed(out_file, **features)\n",
    "    print(\"Saved features to:\", out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: load captions (adapt path/format to your captions file)\n",
    "import re, string\n",
    "\n",
    "def clean_caption_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"[^a-z0-9\\s]\", \"\", txt)   # keep only alphanumerics + spaces\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def load_captions_file(captions_file):\n",
    "    descriptions = {}\n",
    "    with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        # supports common formats: \"img.jpg,caption...\" or \"img.jpg\\tcaption...\"\n",
    "        if '\\t' in line:\n",
    "            img_part, caption = line.split('\\t',1)\n",
    "        else:\n",
    "            parts = line.split(',', 1)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            img_part, caption = parts\n",
    "        img_id = os.path.basename(img_part).split('.')[0]\n",
    "        caption = clean_caption_text(caption)\n",
    "        caption = \"startseq \" + caption + \" endseq\"\n",
    "        descriptions.setdefault(img_id, []).append(caption)\n",
    "    return descriptions\n",
    "\n",
    "# Usage\n",
    "# descriptions = load_captions_file(\"/path/to/Flickr8k/captions.txt\")\n",
    "# print(\"Example:\", list(descriptions.items())[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: tokenizer + stats\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(descriptions, num_words=None, oov_token='unk'):\n",
    "    all_caps = [c for caps in descriptions.values() for c in caps]\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(all_caps)\n",
    "    return tokenizer\n",
    "\n",
    "def get_max_length(descriptions):\n",
    "    all_caps = [c for caps in descriptions.values() for c in caps]\n",
    "    return max(len(c.split()) for c in all_caps)\n",
    "\n",
    "# Example usage (after descriptions built)\n",
    "# tokenizer = create_tokenizer(descriptions, num_words=10000)\n",
    "# vocab_size = min(len(tokenizer.word_index) + 1, 10000)  # if num_words set, cap vocab\n",
    "# max_length = get_max_length(descriptions)\n",
    "# print(\"Vocab size:\", vocab_size)\n",
    "# print(\"Max caption length:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded captions for 8092 images\n"
     ]
    }
   ],
   "source": [
    "captions_file = r\"C:\\Users\\uamaha01\\Downloads\\AI project\\captions.txt\\captions.txt\"  # update path if needed\n",
    "descriptions = load_captions_file(captions_file)\n",
    "print(\"Loaded captions for\", len(descriptions), \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   0%|          | 0/8091 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 8091/8091 [23:37<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to: C:\\Users\\uamaha01\\Downloads\\AI project\\flickr8k_inception_features.npz\n",
      "sample feature keys: ['1000268201_693b08cb0e', '1001773457_577c3a7d70', '1002674143_1b742ab4b8', '1003163366_44323f5815', '1007129816_e794419615']\n",
      "sample captions: ['startseq a child in a pink dress is climbing up a set of stairs in an entry way endseq', 'startseq a girl going into a wooden building endseq', 'startseq a little girl climbing into a wooden playhouse endseq']\n"
     ]
    }
   ],
   "source": [
    "# Extract features from all images in the folder and save to .npz\n",
    "extract_features_from_dir(\n",
    "    image_dir=r\"C:\\Users\\uamaha01\\Downloads\\AI project\\Images\",\n",
    "    out_file=r\"C:\\Users\\uamaha01\\Downloads\\AI project\\flickr8k_inception_features.npz\"\n",
    ")\n",
    "\n",
    "# Load the features file\n",
    "data = np.load(r\"C:\\Users\\uamaha01\\Downloads\\AI project\\flickr8k_inception_features.npz\", allow_pickle=True)\n",
    "print(\"sample feature keys:\", data.files[:5])\n",
    "\n",
    "# Print a sample caption (make sure 'descriptions' is loaded)\n",
    "sample_id = data.files[0]\n",
    "print(\"sample captions:\", descriptions.get(sample_id, [])[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after that need to be confirmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8833\n",
      "Max caption length: 38\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer and get vocabulary size\n",
    "tokenizer = create_tokenizer(descriptions, num_words=10000)\n",
    "vocab_size = min(len(tokenizer.word_index) + 1, 10000)\n",
    "max_length = get_max_length(descriptions)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Max caption length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer as tokenizer.pkl\n",
      "Saved max_length as max_length.txt\n"
     ]
    }
   ],
   "source": [
    "# Save tokenizer and max_length for Streamlit app\n",
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('max_length.txt', 'w') as f:\n",
    "    f.write(str(max_length))\n",
    "\n",
    "print(\"Saved tokenizer as tokenizer.pkl\")\n",
    "print(\"Saved max_length as max_length.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_sequences(tokenizer, max_length, descriptions, features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    for img_id, caps in descriptions.items():\n",
    "        feature = features[img_id]\n",
    "        for cap in caps:\n",
    "            seq = tokenizer.texts_to_sequences([cap])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                X1.append(feature)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# Usage example:\n",
    "# features = dict(np.load(r\"C:\\Users\\uamaha01\\Downloads\\AI project\\flickr8k_inception_features.npz\", allow_pickle=True))\n",
    "# X1, X2, y = create_sequences(tokenizer, max_length, descriptions, features, vocab_size)\n",
    "# print(X1.shape, X2.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features shape: (476960, 2048)\n",
      "Input sequence shape: (476960, 38)\n",
      "Output (y) shape: (476960, 8833)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data (skip missing features)\n",
    "def create_sequences(tokenizer, max_length, descriptions, features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    for img_id, caps in descriptions.items():\n",
    "        if img_id not in features:\n",
    "            continue  # skip if image feature is missing\n",
    "        feature = features[img_id]\n",
    "        for cap in caps:\n",
    "            seq = tokenizer.texts_to_sequences([cap])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                X1.append(feature)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# Load features as a dictionary\n",
    "features = dict(np.load(r\"C:\\Users\\uamaha01\\Downloads\\AI project\\flickr8k_inception_features.npz\", allow_pickle=True))\n",
    "\n",
    "# Prepare training data\n",
    "X1, X2, y = create_sequences(tokenizer, max_length, descriptions, features, vocab_size)\n",
    "\n",
    "print(\"Image features shape:\", X1.shape)\n",
    "print(\"Input sequence shape:\", X2.shape)\n",
    "print(\"Output (y) shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,261,248</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8833</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,081</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,261,248\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8833\u001b[0m)      │  \u001b[38;5;34m2,270,081\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,646,977</span> (21.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,646,977\u001b[0m (21.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,646,977</span> (21.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,646,977\u001b[0m (21.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Image feature extractor model\n",
    "inputs1 = Input(shape=(X1.shape[1],))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# Sequence model\n",
    "inputs2 = Input(shape=(X2.shape[1],))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# Decoder (combine)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 291ms/step - loss: 3.8248\n",
      "Epoch 2/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 291ms/step - loss: 3.0763\n",
      "Epoch 3/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 299ms/step - loss: 2.8230\n",
      "Epoch 4/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 277ms/step - loss: 2.6590\n",
      "Epoch 5/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m573s\u001b[0m 307ms/step - loss: 2.5326\n",
      "Epoch 6/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 318ms/step - loss: 2.4352\n",
      "Epoch 7/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 378ms/step - loss: 2.3551\n",
      "Epoch 8/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 374ms/step - loss: 2.2910\n",
      "Epoch 9/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 346ms/step - loss: 2.2382\n",
      "Epoch 10/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 316ms/step - loss: 2.1937\n",
      "Epoch 11/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 306ms/step - loss: 2.1529\n",
      "Epoch 12/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 324ms/step - loss: 2.1185\n",
      "Epoch 13/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 300ms/step - loss: 2.0901\n",
      "Epoch 14/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m582s\u001b[0m 312ms/step - loss: 2.0643\n",
      "Epoch 15/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 303ms/step - loss: 2.0408\n",
      "Epoch 16/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 316ms/step - loss: 2.0191\n",
      "Epoch 17/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 291ms/step - loss: 2.0002\n",
      "Epoch 18/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 317ms/step - loss: 1.9824\n",
      "Epoch 19/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 300ms/step - loss: 1.9667\n",
      "Epoch 20/20\n",
      "\u001b[1m1864/1864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 305ms/step - loss: 1.9527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d863bc7250>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model (this may take a long time depending on your data and hardware)\n",
    "model.fit([X1, X2], y, epochs=20, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\uamaha01\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\uamaha01\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\uamaha01\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\uamaha01\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.1 MB/s  0:00:01\n",
      "Downloading regex-2025.9.1-cp311-cp311-win_amd64.whl (276 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.1 regex-2025.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.545751\n",
      "BLEU-2: 0.374499\n",
      "BLEU-3: 0.255068\n",
      "BLEU-4: 0.169695\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def generate_caption(model, tokenizer, photo_feature, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_feature, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = None\n",
    "        for w, idx in tokenizer.word_index.items():\n",
    "            if idx == yhat:\n",
    "                word = w\n",
    "                break\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "    return in_text.replace('startseq', '').replace('endseq', '').strip()\n",
    "\n",
    "# Prepare references and predictions\n",
    "actual, predicted = [], []\n",
    "for img_id in descriptions.keys():\n",
    "    if img_id not in features:\n",
    "        continue\n",
    "    photo_feature = features[img_id]\n",
    "    if len(photo_feature.shape) == 1:\n",
    "        photo_feature = photo_feature.reshape((1, -1))\n",
    "    y_pred = generate_caption(model, tokenizer, photo_feature, max_length)\n",
    "    references = [cap.split() for cap in descriptions[img_id]]\n",
    "    actual.append(references)\n",
    "    predicted.append(y_pred.split())\n",
    "\n",
    "# Calculate BLEU scores\n",
    "print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 10/50 images...\n",
      "Evaluated 20/50 images...\n",
      "Evaluated 30/50 images...\n",
      "Evaluated 40/50 images...\n",
      "Evaluated 50/50 images...\n",
      "BLEU-1: 0.535276\n",
      "BLEU-2: 0.375026\n",
      "BLEU-3: 0.250480\n",
      "BLEU-4: 0.147616\n",
      "Evaluation time (seconds): 67.23022937774658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption for your image: a boy in a blue shirt kicking a soccer ball\n",
      "Model saved as caption_model.h5\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Caption generation function\n",
    "def generate_caption(model, tokenizer, photo_feature, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_feature, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = None\n",
    "        for w, idx in tokenizer.word_index.items():\n",
    "            if idx == yhat:\n",
    "                word = w\n",
    "                break\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "    return in_text.replace('startseq', '').replace('endseq', '').strip()\n",
    "\n",
    "# --- EVALUATION ON A SMALL SUBSET FOR SPEED ---\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "actual, predicted = [], []\n",
    "img_ids = list(descriptions.keys())[:50]  # Evaluate on first 50 images for speed\n",
    "\n",
    "for idx, img_id in enumerate(img_ids):\n",
    "    if img_id not in features:\n",
    "        continue\n",
    "    photo_feature = features[img_id]\n",
    "    if len(photo_feature.shape) == 1:\n",
    "        photo_feature = photo_feature.reshape((1, -1))\n",
    "    y_pred = generate_caption(model, tokenizer, photo_feature, max_length)\n",
    "    references = [cap.split() for cap in descriptions[img_id]]\n",
    "    actual.append(references)\n",
    "    predicted.append(y_pred.split())\n",
    "    if (idx+1) % 10 == 0:\n",
    "        print(f\"Evaluated {idx+1}/{len(img_ids)} images...\")\n",
    "\n",
    "print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"Evaluation time (seconds):\", time.time() - start)\n",
    "\n",
    "# --- GENERATE CAPTION FOR A NEW IMAGE ---\n",
    "# Example: change the path to your test image\n",
    "img_path = r\"C:\\Users\\uamaha01\\Downloads\\AI project\\activities-for-younger-kids_narrow.jpg\"\n",
    "img = load_img(img_path, target_size=target_size)\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_fn(x)\n",
    "photo_feature = cnn_model.predict(x, verbose=0)\n",
    "caption = generate_caption(model, tokenizer, photo_feature, max_length)\n",
    "print(\"Generated caption for your image:\", caption)\n",
    "\n",
    "# --- SAVE YOUR MODEL ---\n",
    "model.save(\"caption_model.h5\")\n",
    "print(\"Model saved as caption_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muamaha01\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAI project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcaption_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(r\"C:\\Users\\uamaha01\\Downloads\\AI project\\caption_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "IZoL1B1NNyIJ",
    "FxOzt49VXjjp",
    "tySmqn_NWn5v",
    "R0d1e5t-dcGx"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
